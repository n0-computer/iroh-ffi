namespace iroh {
  /// Set the logging level.
  void set_log_level(LogLevel level);
  /// Initialize the global metrics collection.
  [Throws=IrohError]
  void start_metrics_collection();
  /// Helper function that creates a document key from a canonicalized path, removing the `root` and adding the `prefix`, if they exist
  ///
  /// Appends the null byte to the end of the key.
  [Throws=IrohError]
  bytes path_to_key(string path, string? prefix, string? root);
  /// Helper function that translates a key that was derived from the [`path_to_key`] function back
  /// into a path.
  ///
  /// If `prefix` exists, it will be stripped before converting back to a path
  /// If `root` exists, will add the root as a parent to the created path
  /// Removes any null byte that has been appened to the key
  [Throws=IrohError]
  string key_to_path(bytes key, string? prefix, string? root);
};

/// The logging level. See the rust (log crate)[https://docs.rs/log] for more information.
enum LogLevel {
  "Trace",
  "Debug",
  "Info",
  "Warn",
  "Error",
  "Off",
};

/// An Iroh node. Allows you to sync, store, and transfer data.
interface IrohNode {
  /// Create a new iroh node. The `path` param should be a directory where we can store or load
  /// iroh data from a previous session.
  [Throws=IrohError]
  constructor(string path);
  /// Create a new iroh node with options.
  [Name=with_options,Throws=IrohError]
  constructor(string path, NodeOptions opts);
  /// The string representation of the PublicKey of this node.
  string node_id();

  /// Create a new doc.
  [Throws=IrohError]
  Doc doc_create();
  /// Delete a document from the local node.
  ///
  /// This is a destructive operation. Both the document secret key and all entries in the
  /// document will be permanently deleted from the node's storage. Content blobs will be
  /// deleted through garbage collection unless they are referenced from another document or tag.
  [Throws=IrohError]
  void doc_drop(string doc_id);
  /// Join and sync with an already existing document.
  [Throws=IrohError]
  Doc doc_join(string ticket);
  /// List all the docs we have access to on this node.
  [Throws=IrohError]
  sequence<NamespaceAndCapability> doc_list();
  /// Get a [`Doc`].
  ///
  /// Returns None if the document cannot be found.
  [Throws=IrohError]
  Doc? doc_open(string id);

  /// Create a new author.
  [Throws=IrohError]
  AuthorId author_create();
  /// List all the AuthorIds that exist on this node.
  [Throws=IrohError]
  sequence<AuthorId> author_list();

  /// Get statistics of the running node.
  [Throws=IrohError]
  record<string, CounterStats> stats();
  /// Return `ConnectionInfo`s for each connection we have to another iroh node.
  [Throws=IrohError]
  sequence<ConnectionInfo> connections();
  // Return connection information on the currently running node.
  [Throws=IrohError]
  ConnectionInfo? connection_info([ByRef] PublicKey node_id);
  /// Get status information about a node
  [Throws=IrohError]
  NodeStatusResponse status();

  /// List all complete blobs.
  ///
  /// Note: this allocates for each `BlobListResponse`, if you have many `BlobListReponse`s this may be a prohibitively large list.
  /// Please file an [issue](https://github.com/n0-computer/iroh-ffi/issues/new) if you run into this issue
  [Throws=IrohError]
  sequence<Hash> blobs_list();
  /// Get the size information on a single blob.
  [Throws=IrohError]
  u64 blobs_size([ByRef] Hash hash);
  /// Create a ticket for sharing a blob or collection from this node.
  [Throws=IrohError]
  string blobs_share(Hash hash, BlobFormat blob_format, ShareTicketOptions ticket_options);
  /// Read all bytes of single blob.
  ///
  /// This allocates a buffer for the full blob. Use only if you know that the blob you're
  /// reading is small. If not sure, use [`Self::blobs_size`] and check the size with
  /// before calling [`Self::blobs_read_to_bytes`].
  [Throws=IrohError]
  bytes blobs_read_to_bytes(Hash hash);
  /// Read all bytes of single blob at `offset` for length `len`.
  ///
  /// This allocates a buffer for the full length `len`. Use only if you know that the blob you're
  /// reading is small. If not sure, use [`Self::blobs_size`] and check the size with
  /// before calling [`Self::blobs_read_at_to_bytes`].
  [Throws=IrohError]
  bytes blobs_read_at_to_bytes(Hash hash, u64 offset, u64? len);
  /// Import a blob from a filesystem path.
  ///
  /// `path` should be an absolute path valid for the file system on which
  /// the node runs.
  /// If `in_place` is true, Iroh will assume that the data will not change and will share it in
  /// place without copying to the Iroh data directory.
  [Throws=IrohError]
  void blobs_add_from_path(string path, boolean in_place, SetTagOption tag, WrapOption wrap, AddCallback cb);
  /// Export the blob contents to a file path
  /// The `path` field is expected to be the absolute path.
  [Throws=IrohError]
  void blobs_write_to_path(Hash hash, string path);
  /// Write a blob by passing bytes.
  [Throws=IrohError]
  BlobAddOutcome blobs_add_bytes(bytes bytes);
  /// Download a blob from another node and add it to the local database.
  [Throws=IrohError]
  void blobs_download(BlobDownloadRequest req, DownloadCallback cb);
  /// Download a blob from another node and add it to the local database.
  [Throws=IrohError]
  void blobs_export(Hash hash, string destination, BlobExportFormat format, BlobExportMode mode);
  /// List all incomplete (partial) blobs.
  ///
  /// Note: this allocates for each `BlobListIncompleteResponse`, if you have many `BlobListIncompleteResponse`s this may be a prohibitively large list.
  /// Please file an [issue](https://github.com/n0-computer/iroh-ffi/issues/new) if you run into this issue
  [Throws=IrohError]
  sequence<BlobListIncompleteResponse> blobs_list_incomplete();
  /// List all collections.
  ///
  /// Note: this allocates for each `BlobListCollectionsResponse`, if you have many `BlobListCollectionsResponse`s this may be a prohibitively large list.
  /// Please file an [issue](https://github.com/n0-computer/iroh-ffi/issues/new) if you run into this issue
  [Throws=IrohError]
  sequence<BlobListCollectionsResponse> blobs_list_collections();
  /// Read the content of a collection
  [Throws=IrohError]
  Collection blobs_get_collection(Hash hash);
  /// Create a collection from already existing blobs.
  ///
  /// To automatically clear the tags for the passed in blobs you can set
  /// `tags_to_delete` on those tags, and they will be deleted once the collection is created.
  [Throws=IrohError]
  HashAndTag blobs_create_collection(Collection collection, SetTagOption tag, sequence<string> tags_to_delete);
  /// Delete a blob.
  [Throws=IrohError]
  void blobs_delete_blob(Hash hash);

  /// List all tags
  ///
  /// Note: this allocates for each `ListTagsResponse`, if you have many `Tags`s this may be a prohibitively large list.
  /// Please file an [issue](https://github.com/n0-computer/iroh-ffi/issues/new) if you run into this issue
  [Throws=IrohError]
  sequence<ListTagsResponse> tags_list();
  /// Delete a tag.
  [Throws=IrohError]
  void tags_delete(bytes name);
};

/// Options passed to [`IrohNode.new`]. Controls the behaviour of an iroh node.
dictionary NodeOptions {
  /// How frequently the blob store should clean up unreferenced blobs, in milliseconds.
  /// Set to 0 to disable gc
  u64? gc_interval_millis;
};

/// The Hash and associated tag of a newly created collection
dictionary HashAndTag {
    /// The hash of the collection
    Hash hash;
    /// The tag of the collection
    bytes tag;
};

/// A `Link` includes a name and a hash for a blob in a collection
dictionary LinkAndName {
    /// The name associated with this [`Hash`]
    string name;
    /// The [`Hash`] of the blob
    Hash link;
};

/// A collection of blobs
///
/// Note that the format is subject to change.
interface Collection {
  /// Create a new empty collection
  constructor();
  /// Add the given blob to the collection
  [Throws=IrohError]
  void push(string name, [ByRef] Hash hash);
  /// Check if the collection is empty
  [Throws=IrohError]
  boolean is_empty();
  /// Get the names of the blobs in this collection
  [Throws=IrohError]
  sequence<string> names();
  /// Get the links to the blobs in this collection
  [Throws=IrohError]
  sequence<Hash> links();
  /// Returns a [`Link`] (the name and the hash), for each blob in the collection.
  [Throws=IrohError]
  sequence<LinkAndName> blobs();
  /// Returns the number of blobs in this collection
  [Throws=IrohError]
  u64 len();
};

/// A response to a list collections request
dictionary ListTagsResponse {
  /// The tag
  bytes name;
  /// The format of the associated blob
  BlobFormat format;
  /// The hash of the associated blob
  Hash hash;
};

/// Information about a direct address.
interface DirectAddrInfo {
  /// Get the reported address
  string addr();
  /// Get the reported latency, if it exists
  duration? latency();
  /// Get the last control message received by this node
  LatencyAndControlMsg? last_control();
  /// Get how long ago the last payload message was received for this node
  duration? last_payload();
};

/// The latency and type of the control message
dictionary LatencyAndControlMsg {
  /// The latency of the control message
  duration latency;
  /// The type of control message, represented as a string
  string control_msg;
};

/// A representation of a mutable, synchronizable key-value store.
interface Doc {
  /// Get the document id of this doc.
  string id();
  /// Close the document.
  [Throws=IrohError]
  void close();
  /// Set the content of a key to a byte array.
  [Throws=IrohError]
  Hash set_bytes([ByRef] AuthorId author, bytes key, bytes value);
  /// Set an entries on the doc via its key, hash, and size.
  [Throws=IrohError]
  void set_hash(AuthorId author, bytes key, Hash hash, u64 size);
  /// Add an entry from an absolute file path
  [Throws=IrohError]
  void import_file(AuthorId author, bytes key, string path, boolean in_place, DocImportFileCallback? cb);
  /// Export an entry as a file to a given absolute path
  [Throws=IrohError]
  void export_file(Entry entry, string path, DocExportFileCallback? cb);
  /// Delete entries that match the given `author` and key `prefix`.
  ///
  /// This inserts an empty entry with the key set to `prefix`, effectively clearing all other
  /// entries whose key starts with or is equal to the given `prefix`.
  ///
  /// Returns the number of entries deleted.
  [Throws=IrohError]
  u64 del(AuthorId author_id, bytes prefix);
  /// Get the latest entry for a key and author.
  [Throws=IrohError]
  Entry? get_one(Query query);
  /// Get entries.
  ///
  /// Note: this allocates for each `Entry`, if you have many `Entry`s this may be a prohibitively large list.
  /// Please file an [issue](https://github.com/n0-computer/iroh-ffi/issues/new) if you run into this issue
  [Throws=IrohError]
  sequence<Entry> get_many(Query query);
  /// Get an entry for a key and author.
  ///
  /// Optionally also get the entry if it is empty (i.e. a deletion marker)
  [Throws=IrohError]
  Entry? get_exact(AuthorId author, bytes key, boolean include_empty);

  /// Share this document with peers over a ticket.
  [Throws=IrohError]
  string share(ShareMode mode);
  /// Start to sync this document with a list of peers.
  [Throws=IrohError]
  void start_sync(sequence<NodeAddr> peers);
  /// Stop the live sync for this document.
  [Throws=IrohError]
  void leave();
  /// Subscribe to events for this document.
  [Throws=IrohError]
  void subscribe(SubscribeCallback cb);
  /// Get status info for this document
  [Throws=IrohError]
  OpenState status();
  /// Set the download policy for this document
  [Throws=IrohError]
  void set_download_policy(DownloadPolicy policy);
  /// Get the download policy for this document
  [Throws=IrohError]
  DownloadPolicy get_download_policy();
};

/// The type of `DocImportProgress` event
enum DocImportProgressType {
  /// An item was found with name `name`, from now on referred to via `id`
  "Found",
  /// We got progress ingesting item `id`.
  "Progress",
  /// We are done ingesting `id`, and the hash is `hash`.
  "IngestDone",
  /// We are done with the whole operation.
  "AllDone",
  /// We got an error and need to abort.
  ///
  /// This will be the last message in the stream.
  "Abort"
};

/// A DocImportProgress event indicating a file was found with name `name`, from now on referred to via `id`
dictionary DocImportProgressFound {
  /// A new unique id for this entry.
  u64 id;
  /// The name of the entry.
  string name;
  /// The size of the entry in bytes.
  u64 size;
};

/// A DocImportProgress event indicating we've made progress ingesting item `id`.
dictionary DocImportProgressProgress {
  /// The unique id of the entry.
  u64 id;
  /// The offset of the progress, in bytes.
  u64 offset;
};

/// A DocImportProgress event indicating we are finished adding `id` to the data store and the hash is `hash`.
dictionary DocImportProgressIngestDone {
  /// The unique id of the entry.
  u64 id;
  /// The hash of the entry.
  Hash hash;
};

/// A DocImportProgress event indicating we are done setting the entry to the doc
dictionary DocImportProgressAllDone {
  /// The key of the entry
  bytes key;
};

/// A DocImportProgress event indicating we got an error and need to abort
dictionary DocImportProgressAbort {
  /// The error message
  string error;
};

/// Progress updates for the doc import file operation.
interface DocImportProgress {
  /// Get the type of event
  DocImportProgressType type();
  /// Return the `DocImportProgressFound` event
  DocImportProgressFound as_found();
  /// Return the `DocImportProgressProgress` event
  DocImportProgressProgress as_progress();
  /// Return the `DocImportProgressDone` event
  DocImportProgressIngestDone as_ingest_done();
  /// Return the `DocImportProgressAllDone`
  DocImportProgressAllDone as_all_done();
  /// Return the `DocImportProgressAbort`
  DocImportProgressAbort as_abort();
};

/// The `progress` method will be called for each `DocImportProgress` event that is
/// emitted during a `doc.import_file()` call. Use the `DocImportProgress.type()`
/// method to check the `DocImportProgressType`
callback interface DocImportFileCallback {
  [Throws=IrohError]
  void progress(DocImportProgress progress);
};

/// The type of `DocExportProgress` event
enum DocExportProgressType {
  /// An item was found with name `name`, from now on referred to via `id`
  "Found",
  /// We got progress exporting item `id`.
  "Progress",
  /// We are finished writing item `id`.
  "Done",
  /// We are done writing the entry to the filesystem
  "AllDone",
  /// We got an error and need to abort.
  ///
  /// This will be the last message in the stream.
  "Abort"
};

/// A DocExportProgress event indicating a file was found with name `name`, from now on referred to via `id`
dictionary DocExportProgressFound {
  /// A new unique id for this entry.
  u64 id;
  /// The hash of the entry.
  Hash hash;
  /// The size of the entry in bytes.
  u64 size;
  /// The path where we are writing the entry
  string outpath;
};

/// A DocExportProgress event indicating we've made progress exporting item `id`.
dictionary DocExportProgressProgress {
  /// The unique id of the entry.
  u64 id;
  /// The offset of the progress, in bytes.
  u64 offset;
};

/// A DocExportProgress event indicating we got an error and need to abort
dictionary DocExportProgressAbort {
  /// The error message
  string error;
};

/// Progress updates for the doc import file operation.
interface DocExportProgress {
  /// Get the type of event
  DocExportProgressType type();
  /// Return the `DocExportProgressFound` event
  DocExportProgressFound as_found();
  /// Return the `DocExportProgressProgress` event
  DocExportProgressProgress as_progress();
  /// Return the `DocExportProgressAbort`
  DocExportProgressAbort as_abort();
};

/// The `progress` method will be called for each `DocExportProgress` event that is
/// emitted during a `doc.export_file()` call. Use the `DocExportProgress.type()`
/// method to check the `DocExportProgressType`
callback interface DocExportFileCallback {
  [Throws=IrohError]
  void progress(DocExportProgress progress);
};

/// The namespace id and CapabilityKind (read/write) of the doc
dictionary NamespaceAndCapability {
  /// The namespace id of the doc
  string namespace;
  /// The capability you have for the doc (read/write)
  CapabilityKind capability;
};

/// Build a Query to search for an entry or entries in a doc.
///
/// Use this with `QueryOptions` to determine sorting, grouping, and pagination.
interface Query {
  /// Query all records.
  ///
  /// If `opts` is `None`, the default values will be used:
  ///     sort_by: SortBy::AuthorKey
  ///     direction: SortDirection::Asc
  ///     offset: None
  ///     limit: None
  [Name=all]
  constructor(QueryOptions? opts);
  /// Query only the latest entry for each key, omitting older entries if the entry was written
  /// to by multiple authors.
  ///
  /// If `opts` is `None`, the default values will be used:
  ///     direction: SortDirection::Asc
  ///     offset: None
  ///     limit: None
  [Name=single_latest_per_key]
  constructor(QueryOptions? opts);
  /// Query all entries for by a single author.
  ///
  /// If `opts` is `None`, the default values will be used:
  ///     sort_by: SortBy::AuthorKey
  ///     direction: SortDirection::Asc
  ///     offset: None
  ///     limit: None
  [Name=author]
  constructor([ByRef] AuthorId author, QueryOptions? opts);
  /// Query all entries that have an exact key.
  ///
  /// If `opts` is `None`, the default values will be used:
  ///     sort_by: SortBy::AuthorKey
  ///     direction: SortDirection::Asc
  ///     offset: None
  ///     limit: None
  [Name=key_exact]
  constructor(bytes key, QueryOptions? opts);
  /// Create a Query for a single key and author.
  [Name=author_key_exact]
  constructor([ByRef] AuthorId author, bytes key);
  /// Create a query for all entries with a given key prefix.
  ///
  /// If `opts` is `None`, the default values will be used:
  ///     sort_by: SortBy::AuthorKey
  ///     direction: SortDirection::Asc
  ///     offset: None
  ///     limit: None
  [Name=key_prefix]
  constructor(bytes prefix, QueryOptions? opts);
  /// Create a query for all entries of a single author with a given key prefix.
  ///
  /// If `opts` is `None`, the default values will be used:
  ///     direction: SortDirection::Asc
  ///     offset: None
  ///     limit: None
  [Name=author_key_prefix]
  constructor([ByRef] AuthorId author, bytes prefix, QueryOptions? opts);
  /// Get the offset for this query (number of entries to skip at the beginning).
  u64 offset();
  /// Get the limit for this query (max. number of entries to emit).
  u64? limit();
};

/// Sort direction
enum SortDirection {
  /// Sort ascending
  "Asc",
  /// Sort descending
  "Desc",
};

/// Kind of capability of the doc.
enum CapabilityKind {
  /// A writable doc
  "Write",
  /// A readable doc
  "Read",
};

/// Fields by which the query can be sorted
enum SortBy {
  /// Fields by which the query can be sorted
  "KeyAuthor",
  /// Fields by which the query can be sorted
  "AuthorKey",
};

/// Intended capability for document share tickets
enum ShareMode {
  /// Read-only access
  "Read",
  /// Write access
  "Write",
};

/// A peer and it's addressing information.
interface NodeAddr {
  /// Create a new [`NodeAddr`] with empty [`AddrInfo`].
  constructor([ByRef] PublicKey node_id, string? relay_url, sequence<string> addresses);
  /// Get the direct addresses of this peer.
  sequence<string> direct_addresses();
  /// Get the relay url of this peer.
  string? relay_url();
  /// Returns true if both NodeAddr's have the same values
  boolean equal([ByRef] NodeAddr other);
};

interface NodeStatusResponse {
    /// The node id and socket addresses of this node.
    NodeAddr node_addr();
    /// The bound listening addresses of the node
    sequence<string> listen_addrs();
    /// The version of the node
    string version();
};

/// The `progress` method will be called for each `SubscribeProgress` event that is
/// emitted during a `node.doc_subscribe`. Use the `SubscribeProgress.type()`
/// method to check the `LiveEvent`
callback interface SubscribeCallback {
  [Throws=IrohError]
  void event(LiveEvent event);
};

/// Identifier for an [`Author`]
interface AuthorId {
  /// Get an [`AuthorId`] from a String
  [Name=from_string, Throws=IrohError]
  constructor(string str);
  /// Return the AuthorId as a string
  string to_string();
  /// Returns true when both AuthorId's have the same value
  boolean equal([ByRef] AuthorId other);
};

/// A single entry in a [`Doc`]
///
/// An entry is identified by a key, its [`AuthorId`], and the [`Doc`]'s
/// namespace id. Its value is the 32-byte BLAKE3 [`hash`]
/// of the entry's content data, the size of this content data, and a timestamp.
interface Entry {
  /// Get the [`AuthorId`] of this entry.
  AuthorId author();
  /// Get the key of this entry.
  bytes key();
  /// Get the namespace id of this entry.
  string namespace();
  /// Get the content_hash of this entry.
  Hash content_hash();
  /// Get the content_length of this entry.
  u64 content_len();
  /// Read all content of an [`Entry`] into a buffer.
  /// This allocates a buffer for the full entry. Use only if you know that the entry you're
  /// reading is small. If not sure, use [`Self::content_len`] and check the size with
  /// before calling [`Self::content_bytes`].
  [Throws=IrohError]
  bytes content_bytes(Doc doc);
};

/// Hash type used throughout Iroh. A blake3 hash.
interface Hash {
  /// Calculate the hash of the provide bytes.
  constructor(bytes buf);
  /// Write the hash to a string
  string to_string();
  /// Returns true if the Hash's have the same value
  boolean equal([ByRef] Hash other);
  /// Bytes of the hash.
  bytes to_bytes();
  /// Create a Hash from its raw bytes representation.
  [Name=from_bytes, Throws=IrohError]
  constructor(bytes bytes);
  /// Make a Hash from hex string
  [Name=from_string, Throws=IrohError]
  constructor(string s);
  /// Convert the hash to a hex string.
  string to_hex();
};

/// The state for an open replica.
dictionary OpenState {
  /// Whether to accept sync requests for this replica.
  boolean sync;
  /// How many event subscriptions are open
  u64 subscribers;
  /// By how many handles the replica is currently held open
  u64 handles;
};

/// Stats counter
dictionary CounterStats {
  /// The counter value
  u32 value;
  /// The counter description
  string description;
};

/// Events informing about actions of the live sync progress
interface LiveEvent {
  /// The type LiveEvent
  LiveEventType type();
  /// For `LiveEventType::InsertLocal`, returns an Entry
  Entry as_insert_local();
  /// For `LiveEventType::InsertRemote`, returns an InsertRemoteEvent
  InsertRemoteEvent as_insert_remote();
  /// For `LiveEventType::ContentReady`, returns a Hash
  Hash as_content_ready();
  /// For `LiveEventType::NeighborUp`, returns a PublicKey
  PublicKey as_neighbor_up();
  /// For `LiveEventType::NeighborDown`, returns a PublicKey
  PublicKey as_neighbor_down();
  /// For `LiveEventType::SyncFinished`, returns a SyncEvent
  SyncEvent as_sync_finished();
};


/// Outcome of an InsertRemove event.
dictionary InsertRemoteEvent {
  /// The peer that sent us the entry.
  PublicKey from;
  /// The inserted entry.
  Entry entry;
  /// If the content is available at the local node
  ContentStatus content_status;
};

/// The type of events that can be emitted during the live sync progress
[Enum]
interface LiveEventType {
  /// A local insertion.
  InsertLocal();
  /// Received a remote insert.
  InsertRemote();
  /// The content of an entry was downloaded and is now available at the local node
  ContentReady();
  /// We have a new neighbor in the swarm.
  NeighborUp();
  /// We lost a neighbor in the swarm.
  NeighborDown();
  /// A set-reconciliation sync finished.
  SyncFinished();
};

/// Whether the content status is available on a node.
[Enum]
interface ContentStatus {
  /// The content is completely available.
  Complete();
  /// The content is partially available.
  Incomplete();
  /// The content is missing.
  Missing();
};

/// Outcome of a sync operation
dictionary SyncEvent {
  /// Peer we synced with
  PublicKey peer;
  /// Origin of the sync exchange
  Origin origin;
  /// Timestamp when the sync finished
  timestamp started;
  /// Timestamp when the sync started
  timestamp finished;
  /// Result of the sync operation. `None` if successfull.
  string? result;
};

/// Why we performed a sync exchange
[Enum]
interface Origin {
  /// public, use a unit variant
  Connect(SyncReason reason);
  /// A peer connected to us and we accepted the exchange
  Accept();
};

/// Why we started a sync request
enum SyncReason {
  /// Direct join request via API
  "DirectJoin",
  /// Peer showed up as new neighbor in the gossip swarm
  "NewNeighbor",
  /// We synced after receiving a sync report that indicated news for us
  "SyncReport",
  /// We received a sync report while a sync was running, so run again afterwars
  "Resync",
};

/// Options for sorting and pagination for using [`Query`]s.
dictionary QueryOptions {
  /// Sort by author or key first.
  ///
  /// Default is [`SortBy::AuthorKey`], so sorting first by author and then by key.
  SortBy sort_by;
  /// Direction by which to sort the entries
  ///
  /// Default is [`SortDirection::Asc`]
  SortDirection direction;
  /// Offset
  u64 offset;
  /// Limit to limit the pagination.
  ///
  /// When the limit is 0, the limit does not exist.
  u64 limit;
};

/// Information about a connection
dictionary ConnectionInfo {
  /// The node identifier of the endpoint. Also a public key.
  PublicKey node_id;
  /// Relay url, if available.
  string? relay_url;
  /// List of addresses at which this node might be reachable, plus any latency information we
  /// have about that address and the last time the address was used.
  sequence<DirectAddrInfo> addrs;
  /// The type of connection we have to the peer, either direct or over relay.
  ConnectionType conn_type;
  /// The latency of the `conn_type`.
  duration? latency;
  /// Duration since the last time this peer was used.
  duration? last_used;
};

/// The type of the connection
enum ConnType {
  /// Indicates you have a UDP connection.
  "Direct",
  /// Indicates you have a relayed connection.
  "Relay",
  /// Indicates you have an unverified UDP connection, and a relay connection for backup.
  "Mixed",
  /// Indicates you have no proof of connection.
  "None",
};

/// The type of connection we have to the node
interface ConnectionType{
  /// Whether connection is direct, relay, mixed, or none
  ConnType type();
  /// Return the socket address if this is a direct connection
  string as_direct();
  /// Return the relay url if this is a relay connection
  string as_relay();
  /// Return the socket address and relay url if this is a mixed connection
  ConnectionTypeMixed as_mixed();
};


/// The socket address and url id of the mixed connection
dictionary ConnectionTypeMixed {
    /// Address of the node
    string addr;
    /// Url of the relay node to which the node is connected
    string relay_url;
};

/// A public key
interface PublicKey {
  /// Represent a PublicKey as a string
  string to_string();
  /// Returns true when both PublicKeys have the same value
  boolean equal([ByRef] PublicKey other);
  /// Represent a PublicKey as a byte slice
  bytes to_bytes();
  /// Get a PublicKey from a string
  [Name=from_string, Throws=IrohError]
  constructor(string s);
  /// Get a PublicKey from a byte slice
  [Name=from_bytes, Throws=IrohError]
  constructor(bytes bytes);
  /// The first 10 bytes of the PublicKey represented as a string
  string fmt_short();
};

/// An option for commands that allow setting a tag
interface SetTagOption {
  /// Indicate you want an automatically generated tag
  [Name=auto]
  constructor();
  /// Indicate you want a named tag
  [Name=named]
  constructor(bytes tag);
};

/// Whether to wrap the added data in a collection.
interface WrapOption {
  /// Indicate you do not wrap the file or directory.
  [Name=no_wrap]
  constructor();
  /// Indicate you want to wrap the file or directory in a colletion, with an optional name
  [Name=wrap]
  constructor(string? name);
};

/// A format identifier
enum BlobFormat {
  /// Raw blob
  "Raw",
  /// A sequence of BLAKE3 hashes
  "HashSeq"
};

/// The different types of AddProgress events
enum AddProgressType {
  /// An item was found with name `name`, from now on referred to via `id`
  "Found",
  /// We got progress ingesting item `id`.
  "Progress",
  /// We are done with `id`, and the hash is `hash`.
  "Done",
  /// We are done with the whole operation.
  "AllDone",
  /// We got an error and need to abort.
  ///
  /// This will be the last message in the stream.
  "Abort"
};

/// An AddProgress event indicating an item was found with name `name`, that can be referred to by `id`
dictionary AddProgressFound {
  /// A new unique id for this entry.
  u64 id;
  /// The name of the entry.
  string name;
  /// The size of the entry in bytes.
  u64 size;
};

/// An AddProgress event indicating we got progress ingesting item `id`.
dictionary AddProgressProgress {
  /// The unique id of the entry.
  u64 id;
  /// The offset of the progress, in bytes.
  u64 offset;
};

/// An AddProgress event indicated we are done with `id` and now have a hash `hash`
dictionary AddProgressDone {
  /// The unique id of the entry.
  u64 id;
  /// The hash of the entry.
  Hash hash;
};

/// An AddProgress event indicating we are done with the the whole operation
dictionary AddProgressAllDone {
  /// The hash of the created data.
  Hash hash;
  /// The format of the added data.
  BlobFormat format;
  /// The tag of the added data.
  bytes tag;
};

/// An AddProgress event indicating we got an error and need to abort
dictionary AddProgressAbort {
  /// The error message
  string error;
};

/// Progress updates for the add operation.
interface AddProgress {
  /// Get the type of event
  AddProgressType type();
  /// Return the `AddProgressFound` event
  AddProgressFound as_found();
  /// Return the `AddProgressProgress` event
  AddProgressProgress as_progress();
  /// Return the `AddProgressDone` event
  AddProgressDone as_done();
  /// Return the `AddAllDone`
  AddProgressAllDone as_all_done();
  /// Return the `AddProgressAbort`
  AddProgressAbort as_abort();
};

/// The `progress` method will be called for each `AddProgress` event that is
/// emitted during a `node.blobs_add_from_path`. Use the `AddProgress.type()`
/// method to check the `AddProgressType`
callback interface AddCallback {
  [Throws=IrohError]
  void progress(AddProgress progress);
};

/// Outcome of a blob add operation.
dictionary BlobAddOutcome {
  /// The hash of the blob
  Hash hash;
  /// The format the blob
  BlobFormat format;
  /// The size of the blob
  u64 size;
  /// The tag of the blob
  bytes tag;
};

/// A token containing everything to get a file from the provider.
///
/// It is a single item which can be easily serialized and deserialized.
interface BlobTicket {
  [Throws=IrohError]
  constructor(string ticket);

  /// The provider to get a file from.
  NodeAddr node_addr();
  /// The format of the blob.
  BlobFormat format();
  /// The hash to retrieve.
  Hash hash();

  /// Turn this ticket into parameters for blobs_download.
  BlobDownloadRequest as_download_request();
};

/// A request to the node to download and share the data specified by the hash.
interface BlobDownloadRequest {
  [Throws=IrohError]
  constructor(Hash hash, BlobFormat format, NodeAddr node, SetTagOption tag);
};

/// The `progress` method will be called for each `DownloadProgress` event that is emitted during
/// a `node.blobs_download`. Use the `DownloadProgress.type()` method to check the
/// `DownloadProgressType` of the event.
callback interface DownloadCallback {
  [Throws=IrohError]
  void progress(DownloadProgress progress);
};

/// Progress updates for the get operation.
interface DownloadProgress {
  /// Get the type of event
  /// note that there is no `as_connected` method, as the `Connected` event has no associated data
  DownloadProgressType type();

  /// Return the `DownloadProgressFound` event
  DownloadProgressFound as_found();
  /// Return the `DownloadProgressFoundLocal` event
  DownloadProgressFoundLocal as_found_local();
  /// Return the `DownloadProgressFoundHashSeq` event
  DownloadProgressFoundHashSeq as_found_hash_seq();
  /// Return the `DownloadProgressProgress` event
  DownloadProgressProgress as_progress();
  /// Return the `DownloadProgressDone` event
  DownloadProgressDone as_done();
  /// Return the `DownloadProgressAllDone` event
  DownloadProgressAllDone as_all_done();
  /// Return the `DownloadProgressAbort`
  DownloadProgressAbort as_abort();
};

/// A DownloadProgress event indicating an item was found with hash `hash`, that can be referred to by `id`
dictionary DownloadProgressFound {
  /// A new unique id for this entry.
  u64 id;
  /// child offset
  u64 child;
  /// The hash of the entry.
  Hash hash;
  /// The size of the entry in bytes.
  u64 size;
};

/// A DownloadProgress event indicating an entry was found locally
dictionary DownloadProgressFoundLocal {
  /// child offset
  u64 child;
  /// The hash of the entry.
  Hash hash;
  /// The size of the entry in bytes.
  u64 size;
  /// The ranges that are available locally.
  RangeSpec valid_ranges;
};

/// The expected format of a hash being exported.
enum BlobExportFormat {
    /// The hash refers to any blob and will be exported to a single file.
    "Blob",
    /// The hash refers to a [`crate::format::collection::Collection`] blob
    /// and all children of the collection shall be exported to one file per child.
    ///
    /// If the blob can be parsed as a [`BlobFormat::HashSeq`], and the first child contains
    /// collection metadata, all other children of the collection will be exported to
    /// a file each, with their collection name treated as a relative path to the export
    /// destination path.
    ///
    /// If the blob cannot be parsed as a collection, the operation will fail.
    "Collection",
};

/// Options when creating a ticket
enum ShareTicketOptions {
  /// Include both the relay URL and the direct addresses.
  "RelayAndAddresses",
  /// Only include the relay URL.
  "Relay",
  /// Only include the direct addresses.
  "Addresses",
};

/// The export mode describes how files will be exported.
///
/// This is a hint to the import trait method. For some implementations, this
/// does not make any sense. E.g. an in memory implementation will always have
/// to copy the file into memory. Also, a disk based implementation might choose
/// to copy small files even if the mode is `Reference`.
enum BlobExportMode {
    /// This mode will copy the file to the target directory.
    ///
    /// This is the safe default because the file can not be accidentally modified
    /// after it has been exported.
    "Copy",
    /// This mode will try to move the file to the target directory and then reference it from
    /// the database.
    ///
    /// This has a large performance and storage benefit, but it is less safe since
    /// the file might be modified in the target directory after it has been exported.
    ///
    /// Stores are allowed to ignore this mode and always copy the file, e.g.
    /// if the file is very small or if the store does not support referencing files.
    "TryReference",
};

/// A chunk range specification as a sequence of chunk offsets
interface RangeSpec {
  /// Checks if this [`RangeSpec`] does not select any chunks in the blob
  boolean is_empty();
  /// Check if this [`RangeSpec`] selects all chunks in the blob
  boolean is_all();
};

/// A DownloadProgress event indicating an item was found with hash `hash`, that can be referred to by `id`
dictionary DownloadProgressFoundHashSeq {
  /// Number of children in the collection, if known.
  u64 children;
  /// The hash of the entry.
  Hash hash;
};

/// A DownloadProgress event indicating we got progress ingesting item `id`.
dictionary DownloadProgressProgress {
  /// The unique id of the entry.
  u64 id;
  /// The offset of the progress, in bytes.
  u64 offset;
};

/// A DownloadProgress event indicated we are done with `id`
dictionary DownloadProgressDone {
  /// The unique id of the entry.
  u64 id;
};

/// A DownloadProgress event indicating we are done with the whole operation
dictionary DownloadProgressAllDone {
  /// The number of bytes written
  u64 bytes_written;
  /// The number of bytes read
  u64 bytes_read;
  /// The time it took to transfer the data
  duration elapsed;
};

/// A DownloadProgress event indicating we got an error and need to abort
dictionary DownloadProgressAbort {
  /// The error message
  string error;
};

/// The kinds of progress events that can occur in a `DownloadProgress`
enum DownloadProgressType {
  /// Initial state of the download.
  "InitialState",
  /// Data was found locally
  "FoundLocal",
  /// A new connection was established.
  "Connected",
  /// An item was found with hash `hash`, from now on referred to via `id`
  "Found",
  /// An item was found with hash `hash`, from now on referred to via `id`
  "FoundHashSeq",
  /// We got progress ingesting item `id`.
  "Progress",
  /// We are done with `id`, and the hash is `hash`.
  "Done",
  /// We are done with the whole operation.
  "AllDone",
  /// We got an error and need to abort.
  ///
  /// This will be the last message in the stream.
  "Abort",
};


/// Filter strategy used in download policies.
interface FilterKind {
  /// Verifies whether this filter matches a given key
  boolean matches(bytes key);
  /// Returns a FilterKind that matches if the contained bytes are a prefix of the key.
  [Name=prefix]
  constructor(bytes prefix);
  /// Returns a FilterKind that matches if the contained bytes and the key are the same.
  [Name=exact]
  constructor(bytes key);
};

/// Download policy to decide which content blobs shall be downloaded.
interface DownloadPolicy {
  /// Download everything
  [Name=everything]
  constructor();
  /// Download nothing
  [Name=nothing]
  constructor();
  /// Download nothing except keys that match the given filters
  [Name=nothing_except]
  constructor(sequence<FilterKind> filters);
  /// Download everything except keys that match the given filters
  [Name=everything_except]
  constructor(sequence<FilterKind> filters);
};

/// A response to a list blobs request
dictionary BlobListResponse {
  /// Location of the blob
  string path;
  /// The hash of the blob
  Hash hash;
  /// The size of the blob
  u64 size;
};

/// A response to a list blobs request
dictionary BlobListIncompleteResponse {
  /// The size we got
  u64 size;
  /// The size we expect
  u64 expected_size;
  /// The hash of the blob
  Hash hash;
};

/// A response to a list collections request
dictionary BlobListCollectionsResponse {
  /// Tag of the collection
  bytes tag;
  /// Hash of the collection
  Hash hash;
  /// Number of children in the collection
  ///
  /// This is an optional field, because the data is not always available.
  u64? total_blobs_count;
  /// Total size of the raw data referred to by all links
  ///
  /// This is an optional field, because the data is not always available.
  u64? total_blobs_size;
};

/// An iroh error. Each IrohError contains a string description.
[Error]
interface IrohError {
  Runtime(string description);
  NodeCreate(string description);
  Doc(string description);
  Author(string description);
  Namespace(string description);
  DocTicket(string description);
  BlobTicket(string description);
  Uniffi(string description);
  Connection(string description);
  Blobs(string description);
  Collection(string description);
  Ipv4Addr(string description);
  Ipv6Addr(string description);
  SocketAddr(string description);
  PublicKey(string description);
  NodeAddr(string description);
  Hash(string description);
  FsUtil(string description);
  Tags(string description);
  Url(string description);
  Entry(string description);
};
