namespace iroh {
  /// Set the logging level.
  void set_log_level(LogLevel level);
  /// Initialize the global metrics collection.
  [Throws=IrohError]
  void start_metrics_collection();
  /// Helper function that creates a document key from a canonicalized path, removing the `root` and adding the `prefix`, if they exist
  ///
  /// Appends the null byte to the end of the key.
  [Throws=IrohError]
  bytes path_to_key(string path, string? prefix, string? root);
  /// Helper function that translates a key that was derived from the [`path_to_key`] function back
  /// into a path.
  ///
  /// If `prefix` exists, it will be stripped before converting back to a path
  /// If `root` exists, will add the root as a parent to the created path
  /// Removes any null byte that has been appened to the key
  [Throws=IrohError]
  string key_to_path(bytes key, string? prefix, string? root);
};

/// The logging level. See the rust (log crate)[https://docs.rs/log] for more information.
enum LogLevel {
  "Trace",
  "Debug",
  "Info",
  "Warn",
  "Error",
  "Off",
};

/// An Iroh node. Allows you to sync, store, and transfer data.
interface IrohNode {
  /// Create a new iroh node. The `path` param should be a directory where we can store or load
  /// iroh data from a previous session.
  [Throws=IrohError]
  constructor(string path);
  /// The string representation of the PublicKey of this node.
  string node_id();

  /// Create a new doc.
  [Throws=IrohError]
  Doc doc_create();
  /// Join and sync with an already existing document.
  [Throws=IrohError]
  Doc doc_join(DocTicket ticket);
  /// List all the docs we have access to on this node.
  [Throws=IrohError]
  sequence<NamespaceAndCapability> doc_list();

  /// Create a new author.
  [Throws=IrohError]
  AuthorId author_create();
  /// List all the AuthorIds that exist on this node.
  [Throws=IrohError]
  sequence<AuthorId> author_list();

  /// Get statistics of the running node.
  [Throws=IrohError]
  record<string, CounterStats> stats();
  /// Return `ConnectionInfo`s for each connection we have to another iroh node.
  [Throws=IrohError]
  sequence<ConnectionInfo> connections();
  // Return connection information on the currently running node.
  [Throws=IrohError]
  ConnectionInfo? connection_info(PublicKey node_id);

  /// List all complete blobs.
  ///
  /// Note: this allocates for each `BlobListResponse`, if you have many `BlobListReponse`s this may be a prohibitively large list.
  /// Please file an [issue](https://github.com/n0-computer/iroh-ffi/issues/new) if you run into this issue
  [Throws=IrohError]
  sequence<Hash> blobs_list();
  /// Get the size information on a single blob.
  [Throws=IrohError]
  u64 blobs_size(Hash hash);
  /// Read all bytes of single blob.
  ///
  /// This allocates a buffer for the full blob. Use only if you know that the blob you're
  /// reading is small. If not sure, use [`Self::blobs_size`] and check the size with
  /// before calling [`Self::blobs_read_to_bytes`].
  [Throws=IrohError]
  bytes blobs_read_to_bytes(Hash hash);
  /// Import a blob from a filesystem path.
  ///
  /// `path` should be an absolute path valid for the file system on which
  /// the node runs.
  /// If `in_place` is true, Iroh will assume that the data will not change and will share it in
  /// place without copying to the Iroh data directory.
  [Throws=IrohError]
  void blobs_add_from_path(string path, boolean in_place, SetTagOption tag, WrapOption wrap, AddCallback cb);
  /// Export the blob contents to a file path
  /// The `path` field is expected to be the absolute path.
  [Throws=IrohError]
  void blobs_write_to_path(Hash hash, string path);
  /// Write a blob by passing bytes.
  [Throws=IrohError]
  BlobAddOutcome blobs_add_bytes(bytes bytes, SetTagOption tag);
  /// Download a blob from another node and add it to the local database.
  [Throws=IrohError]
  void blobs_download(BlobDownloadRequest req, DownloadCallback cb);
  /// List all incomplete (partial) blobs.
  ///
  /// Note: this allocates for each `BlobListIncompleteResponse`, if you have many `BlobListIncompleteResponse`s this may be a prohibitively large list.
  /// Please file an [issue](https://github.com/n0-computer/iroh-ffi/issues/new) if you run into this issue
  [Throws=IrohError]
  sequence<BlobListIncompleteResponse> blobs_list_incomplete();
  /// List all collections.
  ///
  /// Note: this allocates for each `BlobListCollectionsResponse`, if you have many `BlobListCollectionsResponse`s this may be a prohibitively large list.
  /// Please file an [issue](https://github.com/n0-computer/iroh-ffi/issues/new) if you run into this issue
  [Throws=IrohError]
  sequence<BlobListCollectionsResponse> blobs_list_collections();
  /// Delete a blob.
  [Throws=IrohError]
  void blobs_delete_blob(Hash hash);
};

/// A representation of a mutable, synchronizable key-value store.
interface Doc {
  /// Get the document id of this doc.
  NamespaceId id();
  /// Close the document.
  [Throws=IrohError]
  void close(); 
  /// Set the content of a key to a byte array.
  [Throws=IrohError]
  Hash set_bytes(AuthorId author, bytes key, bytes value);
  /// Set an entries on the doc via its key, hash, and size.
  [Throws=IrohError]
  void set_hash(AuthorId author, bytes key, Hash hash, u64 size);
  /// Add an entry from an absolute file path
  [Throws=IrohError]
  void import_file(AuthorId author, bytes key, string path, boolean in_place, DocImportFileCallback? cb);
  /// Export an entry as a file to a given absolute path
  [Throws=IrohError]
  void export_file(Entry entry, string path, DocExportFileCallback? cb);
  /// Get the content size of an [`Entry`]
  [Throws=IrohError]
  u64 size(Entry entry);
  /// Read all content of an [`Entry`] into a buffer.
  /// This allocates a buffer for the full entry. Use only if you know that the entry you're
  /// reading is small. If not sure, use [`Self::size`] and check the size with
  /// before calling [`Self::read_to_bytes`].
  [Throws=IrohError]
  bytes read_to_bytes(Entry entry);
  /// Delete entries that match the given `author` and key `prefix`.
  ///
  /// This inserts an empty entry with the key set to `prefix`, effectively clearing all other
  /// entries whose key starts with or is equal to the given `prefix`.
  ///
  /// Returns the number of entries deleted.
  [Throws=IrohError]
  u64 del(AuthorId author_id, bytes prefix);
  /// Get the latest entry for a key and author.
  [Throws=IrohError]
  Entry? get_one(Query query);
  /// Get entries.
  ///
  /// Note: this allocates for each `Entry`, if you have many `Entry`s this may be a prohibitively large list.
  /// Please file an [issue](https://github.com/n0-computer/iroh-ffi/issues/new) if you run into this issue
  [Throws=IrohError]
  sequence<Entry> get_many(Query query);
  /// Share this document with peers over a ticket.
  [Throws=IrohError]
  DocTicket share(ShareMode mode);
  /// Start to sync this document with a list of peers.
  [Throws=IrohError]
  void start_sync(sequence<NodeAddr> peers);
  /// Stop the live sync for this document.
  [Throws=IrohError]
  void leave();
  /// Subscribe to events for this document.
  [Throws=IrohError]
  void subscribe(SubscribeCallback cb);
  /// Get status info for this document
  [Throws=IrohError]
  OpenState status();
};

/// The type of `DocImportProgress` event
enum DocImportProgressType {
  /// An item was found with name `name`, from now on referred to via `id`
  "Found",
  /// We got progress ingesting item `id`.
  "Progress",
  /// We are done ingesting `id`, and the hash is `hash`.
  "IngestDone",
  /// We are done with the whole operation.
  "AllDone",
  /// We got an error and need to abort.
  ///
  /// This will be the last message in the stream.
  "Abort"
};

/// A DocImportProgress event indicating a file was found with name `name`, from now on referred to via `id`
dictionary DocImportProgressFound {
  /// A new unique id for this entry.
  u64 id;
  /// The name of the entry.
  string name;
  /// The size of the entry in bytes.
  u64 size;
};

/// A DocImportProgress event indicating we've made progress ingesting item `id`.
dictionary DocImportProgressProgress {
  /// The unique id of the entry.
  u64 id;
  /// The offset of the progress, in bytes.
  u64 offset;
};

/// A DocImportProgress event indicating we are finished adding `id` to the data store and the hash is `hash`.
dictionary DocImportProgressIngestDone {
  /// The unique id of the entry.
  u64 id;
  /// The hash of the entry.
  Hash hash;
};

/// A DocImportProgress event indicating we are done setting the entry to the doc
dictionary DocImportProgressAllDone {
  /// The key of the entry
  bytes key;
};

/// A DocImportProgress event indicating we got an error and need to abort
dictionary DocImportProgressAbort {
  /// The error message
  string error;
};

/// Progress updates for the doc import file operation.
interface DocImportProgress {
  /// Get the type of event
  DocImportProgressType type();
  /// Return the `DocImportProgressFound` event
  DocImportProgressFound as_found();
  /// Return the `DocImportProgressProgress` event
  DocImportProgressProgress as_progress();
  /// Return the `DocImportProgressDone` event
  DocImportProgressIngestDone as_ingest_done();
  /// Return the `DocImportProgressAllDone`
  DocImportProgressAllDone as_all_done();
  /// Return the `DocImportProgressAbort`
  DocImportProgressAbort as_abort();
};

/// The `progress` method will be called for each `DocImportProgress` event that is
/// emitted during a `doc.import_file()` call. Use the `DocImportProgress.type()`
/// method to check the `DocImportProgressType`
callback interface DocImportFileCallback {
  [Throws=IrohError]
  void progress(DocImportProgress progress);
};

/// The type of `DocExportProgress` event
enum DocExportProgressType {
  /// An item was found with name `name`, from now on referred to via `id`
  "Found",
  /// We got progress exporting item `id`.
  "Progress",
  /// We are done writing the entry to the filesystem
  "AllDone",
  /// We got an error and need to abort.
  ///
  /// This will be the last message in the stream.
  "Abort"
};

/// A DocExportProgress event indicating a file was found with name `name`, from now on referred to via `id`
dictionary DocExportProgressFound {
  /// A new unique id for this entry.
  u64 id;
  /// The hash of the entry.
  Hash hash;
  /// The key of the entry.
  bytes key;
  /// The size of the entry in bytes.
  u64 size;
  /// The path where we are writing the entry
  string outpath;
};

/// A DocExportProgress event indicating we've made progress exporting item `id`.
dictionary DocExportProgressProgress {
  /// The unique id of the entry.
  u64 id;
  /// The offset of the progress, in bytes.
  u64 offset;
};

/// A DocExportProgress event indicating we got an error and need to abort
dictionary DocExportProgressAbort {
  /// The error message
  string error;
};

/// Progress updates for the doc import file operation.
interface DocExportProgress {
  /// Get the type of event
  DocExportProgressType type();
  /// Return the `DocExportProgressFound` event
  DocExportProgressFound as_found();
  /// Return the `DocExportProgressProgress` event
  DocExportProgressProgress as_progress();
  /// Return the `DocExportProgressAbort`
  DocExportProgressAbort as_abort();
};

/// The `progress` method will be called for each `DocExportProgress` event that is
/// emitted during a `doc.export_file()` call. Use the `DocExportProgress.type()`
/// method to check the `DocExportProgressType`
callback interface DocExportFileCallback {
  [Throws=IrohError]
  void progress(DocExportProgress progress);
};

/// The NamespaceId and CapabilityKind (read/write) of the doc
dictionary NamespaceAndCapability {
  /// The NamespaceId of the doc
  NamespaceId namespace;
  /// The capability you have for the doc (read/write)
  CapabilityKind capability;
};

/// Build a Query to search for an entry or entries in a doc.
///
/// Use this with `QueryOptions` to determine sorting, grouping, and pagination.
interface Query {
  /// Query all records.
  ///
  /// If `opts` is `None`, the default values will be used:
  ///     sort_by: SortBy::AuthorKey
  ///     direction: SortDirection::Asc
  ///     offset: None
  ///     limit: None
  [Name=all]
  constructor(QueryOptions? opts);
  /// Query only the latest entry for each key, omitting older entries if the entry was written
  /// to by multiple authors.
  ///
  /// If `opts` is `None`, the default values will be used:
  ///     direction: SortDirection::Asc
  ///     offset: None
  ///     limit: None
  [Name=single_latest_per_key]
  constructor(QueryOptions? opts);
  /// Query all entries for by a single author.
  ///
  /// If `opts` is `None`, the default values will be used:
  ///     sort_by: SortBy::AuthorKey
  ///     direction: SortDirection::Asc
  ///     offset: None
  ///     limit: None
  [Name=author]
  constructor(AuthorId author, QueryOptions? opts);
  /// Query all entries that have an exact key.
  ///
  /// If `opts` is `None`, the default values will be used:
  ///     sort_by: SortBy::AuthorKey
  ///     direction: SortDirection::Asc
  ///     offset: None
  ///     limit: None
  [Name=key_exact]
  constructor(bytes key, QueryOptions? opts);
  /// Create a Query for a single key and author.
  [Name=author_key_exact]
  constructor(AuthorId author, bytes key);
  /// Create a query for all entries with a given key prefix.
  ///  
  /// If `opts` is `None`, the default values will be used:
  ///     sort_by: SortBy::AuthorKey
  ///     direction: SortDirection::Asc
  ///     offset: None
  ///     limit: None
  [Name=key_prefix]
  constructor(bytes prefix, QueryOptions? opts);
  /// Create a query for all entries of a single author with a given key prefix.
  ///  
  /// If `opts` is `None`, the default values will be used:
  ///     direction: SortDirection::Asc
  ///     offset: None
  ///     limit: None
  [Name=author_key_prefix]
  constructor(AuthorId author, bytes prefix, QueryOptions? opts);
  /// Get the offset for this query (number of entries to skip at the beginning).
  u64 offset();
  /// Get the limit for this query (max. number of entries to emit).
  u64? limit();
};

/// Sort direction
enum SortDirection {
  /// Sort ascending
  "Asc",
  /// Sort descending
  "Desc",
};

/// Kind of capability of the doc.
enum CapabilityKind {
  /// A writable doc
  "Write",
  /// A readable doc
  "Read",
};

/// Fields by which the query can be sorted
enum SortBy {
  /// Fields by which the query can be sorted
  "KeyAuthor",
  /// Fields by which the query can be sorted
  "AuthorKey",
};

/// Intended capability for document share tickets
enum ShareMode {
  /// Read-only access
  "Read",
  /// Write access
  "Write",
};

/// A peer and it's addressing information.
interface NodeAddr {
  /// Create a new [`NodeAddr`] with empty [`AddrInfo`].
  constructor(PublicKey node_id, u16? region_id, sequence<SocketAddr> addresses);
  /// Get the direct addresses of this peer.
  sequence<SocketAddr> direct_addresses();
  /// Get the derp region of this peer.
  u16? derp_region();
  /// Returns true if both NodeAddr's have the same values
  boolean equal(NodeAddr other);
};

/// The `progress` method will be called for each `SubscribeProgress` event that is
/// emitted during a `node.doc_subscribe`. Use the `SubscribeProgress.type()`
/// method to check the `LiveEvent`
callback interface SubscribeCallback {
  [Throws=IrohError]
  void event(LiveEvent event);
};

/// Identifier for an [`Author`]
interface AuthorId {
  /// Get an [`AuthorId`] from a String
  [Name=from_string, Throws=IrohError]
  constructor(string str);
  /// Return the AuthorId as a string
  string to_string();
  /// Returns true when both AuthorId's have the same value
  boolean equal(AuthorId other);
};

/// A single entry in a [`Doc`]
///
/// An entry is identified by a key, its [`AuthorId`], and the [`Doc`]'s
/// [`NamespaceId`]. Its value is the 32-byte BLAKE3 [`hash`]
/// of the entry's content data, the size of this content data, and a timestamp.
interface Entry {
  /// Get the [`AuthorId`] of this entry.
  AuthorId author();
  /// Get the key of this entry.
  bytes key();
  /// Get the [`NamespaceId`] of this entry.
  NamespaceId namespace();
  /// Get the content_hash of this entry.
  Hash content_hash();
  /// Get the content_length of this entry.
  u64 content_len();
};

/// Hash type used throughout Iroh. A blake3 hash.
interface Hash {
  /// Calculate the hash of the provide bytes.
  constructor(bytes buf);
  /// Write the hash to a string
  string to_string();
  /// Returns true if the Hash's have the same value
  boolean equal(Hash other);
  /// Bytes of the hash.
  bytes to_bytes();
  /// Create a Hash from its raw bytes representation.
  [Name=from_bytes, Throws=IrohError]
  constructor(bytes bytes);
  /// Make a Hash from hex or base 64 encoded cid string
  [Name=from_string, Throws=IrohError]
  constructor(string str);
  /// Get the cid as bytes.
  bytes as_cid_bytes();
  /// Try to create a blake3 cid from cid bytes.
  ///
  /// This will only work if the prefix is the following:
  /// - version 1
  /// - raw codec
  /// - blake3 hash function
  /// - 32 byte hash size
  [Name=from_cid_bytes, Throws=IrohError]
  constructor(bytes bytes);
  /// Convert the hash to a hex string.
  string to_hex();
};

/// Contains both a key (either secret or public) to a document, and a list of peers to join.
interface DocTicket {
  /// Create a `DocTicket` from a string
  [Name=from_string, Throws=IrohError]
  constructor(string content);
  /// Return a string representation of a `DocTicket`
  string to_string();
  /// Returns true if both `DocTicket`'s have the same value
  boolean equal(DocTicket other);
};

/// The state for an open replica.
dictionary OpenState {
  /// Whether to accept sync requests for this replica.
  boolean sync;
  /// How many event subscriptions are open
  u64 subscribers;
  /// By how many handles the replica is currently held open
  u64 handles;
};

/// Stats counter
dictionary CounterStats {
  /// The counter value
  u64 value;
  /// The counter description
  string description;
};

/// Events informing about actions of the live sync progress
interface LiveEvent {
  /// The type LiveEvent
  LiveEventType type();
  /// For `LiveEventType::InsertLocal`, returns an Entry
  Entry as_insert_local();
  /// For `LiveEventType::InsertRemote`, returns an InsertRemoteEvent
  InsertRemoteEvent as_insert_remote();
  /// For `LiveEventType::ContentReady`, returns a Hash
  Hash as_content_ready();
  /// For `LiveEventType::NeighborUp`, returns a PublicKey
  PublicKey as_neighbor_up();
  /// For `LiveEventType::NeighborDown`, returns a PublicKey
  PublicKey as_neighbor_down();
  /// For `LiveEventType::SyncFinished`, returns a SyncEvent
  SyncEvent as_sync_finished();
};


/// Outcome of an InsertRemove event.
dictionary InsertRemoteEvent {
  /// The peer that sent us the entry.
  PublicKey from;
  /// The inserted entry.
  Entry entry;
  /// If the content is available at the local node
  ContentStatus content_status;
};

/// The type of events that can be emitted during the live sync progress
[Enum]
interface LiveEventType {
  /// A local insertion.
  InsertLocal();
  /// Received a remote insert.
  InsertRemote();
  /// The content of an entry was downloaded and is now available at the local node
  ContentReady();
  /// We have a new neighbor in the swarm.
  NeighborUp();
  /// We lost a neighbor in the swarm.
  NeighborDown();
  /// A set-reconciliation sync finished.
  SyncFinished();
};

/// Whether the content status is available on a node.
[Enum]
interface ContentStatus {
  /// The content is completely available.
  Complete();
  /// The content is partially available.
  Incomplete();
  /// The content is missing.
  Missing();
};

/// An identifier for a Doc
interface NamespaceId {
  /// Get an [`NamespaceId`] from a String
  [Name=from_string, Throws=IrohError]
  constructor(string str);
  /// Return a NamespaceId as a string
  string to_string();
  /// Returns true when both NamespaceId's have the same value
  boolean equal(NamespaceId other);
};

/// Outcome of a sync operation
dictionary SyncEvent {
  /// Peer we synced with
  PublicKey peer;
  /// Origin of the sync exchange
  Origin origin;
  /// Timestamp when the sync finished
  timestamp started;
  /// Timestamp when the sync started
  timestamp finished;
  /// Result of the sync operation. `None` if successfull.
  string? result;
};

/// Why we performed a sync exchange
[Enum]
interface Origin {
  /// public, use a unit variant
  Connect(SyncReason reason);
  /// A peer connected to us and we accepted the exchange
  Accept();
};

/// Why we started a sync request
enum SyncReason {
  /// Direct join request via API
  "DirectJoin",
  /// Peer showed up as new neighbor in the gossip swarm
  "NewNeighbor",
  /// We synced after receiving a sync report that indicated news for us
  "SyncReport",
  /// We received a sync report while a sync was running, so run again afterwars
  "Resync",
};

/// Options for sorting and pagination for using [`Query`]s.
dictionary QueryOptions {
  /// Sort by author or key first.
  ///
  /// Default is [`SortBy::AuthorKey`], so sorting first by author and then by key.
  SortBy sort_by;
  /// Direction by which to sort the entries
  ///
  /// Default is [`SortDirection::Asc`]
  SortDirection direction;
  /// Offset
  u64 offset;
  /// Limit to limit the pagination.
  ///
  /// When the limit is 0, the limit does not exist.
  u64 limit;
};

/// Information about a connection
dictionary ConnectionInfo {
  /// The public key of the endpoint.
  PublicKey public_key;
  /// Derp region, if available.
  u16? derp_region;
  /// List of addresses at which this node might be reachable, plus any latency information we
  /// have about that address and the last time the address was used.
  sequence<DirectAddrInfo> addrs;
  /// The type of connection we have to the peer, either direct or over relay.
  ConnectionType conn_type;
  /// The latency of the `conn_type`.
  duration? latency;
  /// Duration since the last time this peer was used.
  duration? last_used;
};

/// The type of connection we have to the node
[Enum]
interface ConnectionType{
  /// Direct UDP connection
  Direct(string addr, u16 port);
  /// Relay connection over DERP
  Relay(u16 port);
  /// Both a UDP and a DERP connection are used.
  ///
  /// This is the case if we do have a UDP address, but are missing a recent confirmation that
  /// the address works.
  Mixed(string addr, u16 port);
  /// We have no verified connection to this PublicKey
  None();
};

/// Information about a direct address.
interface DirectAddrInfo {};

/// Type of SocketAddr
enum SocketAddrType {
  /// Ipv4 SocketAddr
  "V4",
  /// Ipv6 SocketAddr
  "V6",
};

/// Ipv4 address
interface Ipv4Addr {
  /// Create a new Ipv4 addr from 4 eight-bit octets
  constructor(u8 a, u8 b, u8 c, u8 d);
  /// Create a new Ipv4 addr from a String
  [Name=from_string, Throws=IrohError]
  constructor(string str);
  /// Returns an Ipv4 address as a string
  string to_string();
  /// Get the 4 octets as bytes
  sequence<u8> octets(); 
  /// Returns true if both Ipv4Addrs have the same value
  boolean equal(Ipv4Addr other);
};

/// An Ipv4 socket address
interface SocketAddrV4 {
  /// Create a new socket address from an [`Ipv4Addr`] and a port number
  constructor(Ipv4Addr ipv4, u16 port);
  /// Create a new Ipv4 addr from a String
  [Name=from_string, Throws=IrohError]
  constructor(string str);
  /// Returns a Ipv4 SocketAddr as string
  string to_string();
  /// Returns the IP address associated with this socket address
  Ipv4Addr ip(); 
  /// Returns the port number associated with this socket address
  u16 port();
  /// Returns true if both SocketAddrV4's have the same value
  boolean equal(SocketAddrV4 other);
};

/// Ipv6 address
interface Ipv6Addr {
  /// Create a new Ipv6 Addr from eight sixteen-bit segments
  constructor(u16 a, u16 b, u16 c, u16 d, u16 e, u16 f, u16 g, u16 h);
  /// Create a new Ipv6 addr from a String
  [Name=from_string, Throws=IrohError]
  constructor(string str);
  /// Returns a Ipv6 Address as a string
  string to_string();
  /// Get the 8 sixteen-bit segments as an array
  sequence<u16> segments(); 
  /// Returns true if both Ipv6Addr's have the same value
  boolean equal(Ipv6Addr other);
};

/// An Ipv6 socket address
interface SocketAddrV6 {
  /// Create a new socket address from an [`Ipv6Addr`] and a port number
  constructor(Ipv6Addr ipv6, u16 port);
  /// Create a new Ipv6 addr from a String
  [Name=from_string, Throws=IrohError]
  constructor(string str);
  /// Returns the Ipv6 SocketAddr as a string
  string to_string();
  /// Returns the IP address associated with this socket address
  Ipv6Addr ip(); 
  /// Returns the port number associated with this socket address
  u16 port();
  /// Returns true if both SocketAddrV6's have the same value
  boolean equal(SocketAddrV6 other);
};

/// An internet socket address, either Ipv4 or Ipv6
interface SocketAddr {
  /// Create an Ipv4 SocketAddr
  [Name=from_ipv4]
  constructor(Ipv4Addr ipv4, u16 port);
  /// Create an Ipv6 SocketAddr
  [Name=from_ipv6]
  constructor(Ipv6Addr ipv6, u16 port);
  /// The type of SocketAddr
  SocketAddrType type();
  /// Get the IPv4 SocketAddr representation
  SocketAddrV4 as_ipv4();
  /// Get the IPv6 SocketAddr representation
  SocketAddrV6 as_ipv6();
  /// Returns true if the two SocketAddrs have the same value
  boolean equal(SocketAddr other);
};

/// A public key
interface PublicKey {
  /// Represent a PublicKey as a string
  string to_string();
  /// Returns true when both PublicKeys have the same value
  boolean equal(PublicKey other);
  /// Represent a PublicKey as a byte slice
  bytes to_bytes();
  /// Get a PublicKey from a string
  [Name=from_string, Throws=IrohError]
  constructor(string s); 
  /// Get a PublicKey from a byte slice
  [Name=from_bytes, Throws=IrohError]
  constructor(bytes bytes);
  /// The first 10 bytes of the PublicKey represented as a string
  string fmt_short();
};

/// A tag
interface Tag {
  /// Represent a Tag as a string
  string to_string();
  /// Returns trun when the two tags have the same value
  boolean equal(Tag other);
  /// Represent a Tag as bytes
  bytes to_bytes();
  /// Get a Tag from a string
  [Name=from_string]
  constructor(string s); 
  /// Get a Tag from a slice of bytes
  [Name=from_bytes]
  constructor(bytes bytes);
};

/// An option for commands that allow setting a Tag
interface SetTagOption {
  /// Indicate you want an automatically generated tag
  [Name=auto]
  constructor();
  /// Indicate you want a named tag
  [Name=named]
  constructor(Tag tag);
};

/// Whether to wrap the added data in a collection.
interface WrapOption {
  /// Indicate you do not wrap the file or directory.
  [Name=no_wrap]
  constructor();
  /// Indicate you want to wrap the file or directory in a colletion, with an optional name
  [Name=wrap]
  constructor(string? name);
};

/// A format identifier
enum BlobFormat {
  /// Raw blob
  "Raw",
  /// A sequence of BLAKE3 hashes
  "HashSeq"
};

/// The different types of AddProgress events
enum AddProgressType {
  /// An item was found with name `name`, from now on referred to via `id`
  "Found",
  /// We got progress ingesting item `id`.
  "Progress",
  /// We are done with `id`, and the hash is `hash`.
  "Done",
  /// We are done with the whole operation.
  "AllDone",
  /// We got an error and need to abort.
  ///
  /// This will be the last message in the stream.
  "Abort"
};

/// An AddProgress event indicating an item was found with name `name`, that can be referred to by `id`
dictionary AddProgressFound {
  /// A new unique id for this entry.
  u64 id;
  /// The name of the entry.
  string name;
  /// The size of the entry in bytes.
  u64 size;
};

/// An AddProgress event indicating we got progress ingesting item `id`.
dictionary AddProgressProgress {
  /// The unique id of the entry.
  u64 id;
  /// The offset of the progress, in bytes.
  u64 offset;
};

/// An AddProgress event indicated we are done with `id` and now have a hash `hash`
dictionary AddProgressDone {
  /// The unique id of the entry.
  u64 id;
  /// The hash of the entry.
  Hash hash;
};

/// An AddProgress event indicating we are done with the the whole operation
dictionary AddProgressAllDone {
  /// The hash of the created data.
  Hash hash;
  /// The format of the added data.
  BlobFormat format;
  /// The tag of the added data.
  Tag tag;
};

/// An AddProgress event indicating we got an error and need to abort
dictionary AddProgressAbort {
  /// The error message
  string error;
};

/// Progress updates for the add operation.
interface AddProgress {
  /// Get the type of event
  AddProgressType type();
  /// Return the `AddProgressFound` event
  AddProgressFound as_found();
  /// Return the `AddProgressProgress` event
  AddProgressProgress as_progress();
  /// Return the `AddProgressDone` event
  AddProgressDone as_done();
  /// Return the `AddAllDone`
  AddProgressAllDone as_all_done();
  /// Return the `AddProgressAbort`
  AddProgressAbort as_abort();
};

/// The `progress` method will be called for each `AddProgress` event that is
/// emitted during a `node.blobs_add_from_path`. Use the `AddProgress.type()`
/// method to check the `AddProgressType`
callback interface AddCallback {
  [Throws=IrohError]
  void progress(AddProgress progress);
};

/// Outcome of a blob add operation.
dictionary BlobAddOutcome {
  /// The hash of the blob
  Hash hash;
  /// The format the blob
  BlobFormat format;
  /// The size of the blob
  u64 size;
  /// The tag of the blob
  Tag tag;
};

/// A Request token is an opaque byte sequence associated with a single request.
/// Applications can use request tokens to implement request authorization,
/// user association, etc.
interface RequestToken {
  /// Creates a new request token from bytes.
  [Throws=IrohError]
  constructor(bytes bytes); 
  /// Generate a random 32 byte request token.
  [Name=generate]
  constructor();
  /// Returns a reference the token bytes.
  bytes as_bytes();
  /// Create a request token from a string
  [Name=from_string, Throws=IrohError]
  constructor(string str);
  /// Returns true if both RequestTokens have the same value
  boolean equal(RequestToken other);
};

/// Location to store a downloaded blob at.
interface DownloadLocation {
  /// Store in the node's blob storage directory.
  [Name=internal]
  constructor();
  /// Store at the provided path.
  ///
  /// If `in_place` is true, the data is shared in place, i.e. it is moved to the
  /// out path instead of being copied. The database itself contains only a
  /// reference to the out path of the file.
  ///
  /// If the data is modified in the location specified by the out path,
  /// download attempts for the associated hash will fail.
  [Name=external]
  constructor(string path, boolean in_place);
};

/// A request to the node to download and share the data specified by the hash.
interface BlobDownloadRequest {
  constructor(Hash hash, BlobFormat format, NodeAddr node, SetTagOption tag, DownloadLocation out, RequestToken? token);
};

/// The `progress` method will be called for each `DownloadProgress` event that is emitted during
/// a `node.blobs_download`. Use the `DownloadProgress.type()` method to check the
/// `DownloadProgressType` of the event.
callback interface DownloadCallback {
  [Throws=IrohError]
  void progress(DownloadProgress progress);
};

/// Progress updates for the get operation.
interface DownloadProgress {
  /// Get the type of event
  DownloadProgressType type();
  /// Return the `DownloadProgressFound` event
  DownloadProgressFound as_found();
  /// Return the `DownloadProgressFoundHashSeq` event
  DownloadProgressFoundHashSeq as_found_hash_seq();
  /// Return the `DownloadProgressProgress` event
  DownloadProgressProgress as_progress();
  /// Return the `DownloadProgressDone` event
  DownloadProgressDone as_done();
  /// Return the `DownloadProgressNetworkDone` event
  DownloadProgressNetworkDone as_network_done();
  /// Return the `DownloadProgressExport` event
  DownloadProgressExport as_export();
  /// Return the `DownloadProgressExportProgress` event
  DownloadProgressExportProgress as_export_progress();
  /// Return the `DownloadProgressAbort`
  DownloadProgressAbort as_abort();
};

/// A DownloadProgress event indicating an item was found with hash `hash`, that can be referred to by `id`
dictionary DownloadProgressFound {
  /// A new unique id for this entry.
  u64 id;
  /// child offset
  u64 child;
  /// The hash of the entry.
  Hash hash;
  /// The size of the entry in bytes.
  u64 size;
};

/// A DownloadProgress event indicating an item was found with hash `hash`, that can be referred to by `id`
dictionary DownloadProgressFoundHashSeq {
  /// Number of children in the collection, if known.
  u64 children;
  /// The hash of the entry.
  Hash hash;
};

/// A DownloadProgress event indicating we got progress ingesting item `id`.
dictionary DownloadProgressProgress {
  /// The unique id of the entry.
  u64 id;
  /// The offset of the progress, in bytes.
  u64 offset;
};

/// A DownloadProgress event indicated we are done with `id`
dictionary DownloadProgressDone {
  /// The unique id of the entry.
  u64 id;
};

/// A DownloadProgress event indicating we are done with the networking portion - all data is local
dictionary DownloadProgressNetworkDone {
  /// The number of bytes written
  u64 bytes_written;
  /// The number of bytes read
  u64 bytes_read;
  /// The time it took to transfer the data
  duration elapsed;
};

/// A DownloadProgress event indicating We have made progress exporting the data.
///
/// This is only sent for large blobs.
dictionary DownloadProgressExport {
  /// Unique id of the entry that is being exported.
  u64 id;
  /// The hash of the entry
  Hash hash;
  /// The size of the entry in bytes
  u64 size;
  /// The path to the file where the data is exported
  string target;
};

/// A DownloadProgress event indicating We have made progress exporting the data.
///
/// This is only sent for large blobs.
dictionary DownloadProgressExportProgress {
  /// Unique id of the entry that is being exported.
  u64 id;
  /// The offset of the progress, in bytes.
  u64 offset;
};

/// A DownloadProgress event indicating we got an error and need to abort
dictionary DownloadProgressAbort {
  /// The error message
  string error;
};

/// The kinds of progress events that can occur in a `DownloadProgress`
enum DownloadProgressType {
  /// A new connection was established.
  "Connected",
  /// An item was found with hash `hash`, from now on referred to via `id`
  "Found",
  /// An item was found with hash `hash`, from now on referred to via `id`
  "FoundHashSeq",
  /// We got progress ingesting item `id`.
  "Progress",
  /// We are done with `id`, and the hash is `hash`.
  "Done",
  /// We are done with the network part - all data is local
  "NetworkDone",
  /// The download part is done for this id, we are not exporting the data to the specified outpath
  "Export",
  /// We have made progress exporting the data
  ///
  /// This is only sent for large blobs"Export",
  "ExportProgress",
  /// We are done with the whole operation.
  "AllDone",
  /// We got an error and need to abort.
  ///
  /// This will be the last message in the stream.
  "Abort",
};

/// A response to a list blobs request
dictionary BlobListResponse {
  /// Location of the blob
  string path;
  /// The hash of the blob
  Hash hash;
  /// The size of the blob
  u64 size;
};
    
/// A response to a list blobs request
dictionary BlobListIncompleteResponse {
  /// The size we got
  u64 size;
  /// The size we expect
  u64 expected_size;
  /// The hash of the blob
  Hash hash;
};

/// A response to a list collections request
dictionary BlobListCollectionsResponse {
  /// Tag of the collection
  Tag tag;
  /// Hash of the collection
  Hash hash;
  /// Number of children in the collection
  ///
  /// This is an optional field, because the data is not always available.
  u64? total_blobs_count;
  /// Total size of the raw data referred to by all links
  ///
  /// This is an optional field, because the data is not always available.
  u64? total_blobs_size;
};

/// An iroh error. Each IrohError contains a string description.
[Error]
interface IrohError {
  Runtime(string description);
  NodeCreate(string description);
  Doc(string description);
  Author(string description);
  Namespace(string description);
  DocTicket(string description);
  Uniffi(string description);
  Connection(string description);
  Blobs(string description);
  Ipv4Addr(string description);
  Ipv6Addr(string description);
  SocketAddrV4(string description);
  SocketAddrV6(string description);
  PublicKey(string description);
  NodeAddr(string description);
  Hash(string description);
  RequestToken(string description);
  FsUtil(string description);
};
